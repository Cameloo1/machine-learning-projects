{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training & Evaluation Notebook\n",
        "\n",
        "This notebook trains a gradient-boosted classifier to predict alert priority (low/medium/high) based on SOC-style alert features.\n",
        "\n",
        "## Steps:\n",
        "1. Generate synthetic alert dataset\n",
        "2. Split into train/validation sets\n",
        "3. Build preprocessing pipeline + model\n",
        "4. Train and evaluate\n",
        "5. Save model, metadata, and metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import json\n",
        "import joblib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Paths\n",
        "BASE_DIR = Path(__file__).parent.parent\n",
        "DATA_DIR = BASE_DIR / \"data\"\n",
        "MODELS_DIR = BASE_DIR / \"models\"\n",
        "METRICS_DIR = BASE_DIR / \"metrics\"\n",
        "\n",
        "# Create directories if they don't exist\n",
        "DATA_DIR.mkdir(exist_ok=True)\n",
        "MODELS_DIR.mkdir(exist_ok=True)\n",
        "METRICS_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "print(\"Setup complete!\")\n",
        "print(f\"Base directory: {BASE_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Generate Synthetic Alert Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if dataset already exists\n",
        "csv_path = DATA_DIR / \"alerts_sample.csv\"\n",
        "\n",
        "if csv_path.exists():\n",
        "    print(f\"Dataset already exists at {csv_path}\")\n",
        "    print(\"Skipping generation. Delete the file to regenerate.\")\n",
        "else:\n",
        "    print(\"Generating synthetic alert dataset...\")\n",
        "    \n",
        "    # Generate ~6000-7000 rows (random between 5000-8000)\n",
        "    n_samples = np.random.randint(5000, 8001)\n",
        "    print(f\"Generating {n_samples} samples...\")\n",
        "    \n",
        "    # Feature definitions\n",
        "    alert_types = [\"brute_force\", \"malware\", \"suspicious_login\", \"data_exfil\", \"other\"]\n",
        "    asset_criticalities = [\"low\", \"medium\", \"high\"]\n",
        "    \n",
        "    # Initialize lists for features\n",
        "    data = {\n",
        "        \"alert_type\": [],\n",
        "        \"source_ip_risk\": [],\n",
        "        \"user_risk_score\": [],\n",
        "        \"failed_login_count_24h\": [],\n",
        "        \"geo_impossible_travel\": [],\n",
        "        \"asset_criticality\": [],\n",
        "        \"historical_false_positive_rate\": [],\n",
        "        \"priority\": []\n",
        "    }\n",
        "    \n",
        "    for i in range(n_samples):\n",
        "        # Generate base features with some correlation to priority\n",
        "        asset_crit = np.random.choice(asset_criticalities, p=[0.4, 0.4, 0.2])\n",
        "        alert_type = np.random.choice(alert_types)\n",
        "        \n",
        "        # Generate risk scores (correlated with priority)\n",
        "        base_risk = np.random.beta(2, 5)  # Skewed toward lower values\n",
        "        source_ip_risk = base_risk + np.random.normal(0, 0.15)\n",
        "        source_ip_risk = np.clip(source_ip_risk, 0, 1)\n",
        "        \n",
        "        user_risk_score = base_risk + np.random.normal(0, 0.15)\n",
        "        user_risk_score = np.clip(user_risk_score, 0, 1)\n",
        "        \n",
        "        # Failed logins (correlated with priority)\n",
        "        failed_logins = np.random.poisson(2) if np.random.random() > 0.3 else np.random.poisson(10)\n",
        "        failed_logins = max(0, failed_logins)\n",
        "        \n",
        "        # Geo impossible travel (binary, correlated with priority)\n",
        "        geo_impossible = 1 if np.random.random() < 0.15 else 0\n",
        "        \n",
        "        # Historical false positive rate (inverse correlation with priority)\n",
        "        hist_fp_rate = np.random.beta(3, 2)  # Skewed toward higher values\n",
        "        \n",
        "        # Determine priority based on features (realistic rules)\n",
        "        priority_score = 0.0\n",
        "        \n",
        "        # Asset criticality contribution\n",
        "        if asset_crit == \"high\":\n",
        "            priority_score += 0.4\n",
        "        elif asset_crit == \"medium\":\n",
        "            priority_score += 0.2\n",
        "        \n",
        "        # Risk scores contribution\n",
        "        priority_score += (source_ip_risk * 0.2) + (user_risk_score * 0.2)\n",
        "        \n",
        "        # Failed logins contribution\n",
        "        priority_score += min(0.15, failed_logins / 20.0)\n",
        "        \n",
        "        # Geo impossible travel contribution\n",
        "        if geo_impossible == 1:\n",
        "            priority_score += 0.15\n",
        "        \n",
        "        # Historical false positive rate (inverse)\n",
        "        priority_score += (1 - hist_fp_rate) * 0.1\n",
        "        \n",
        "        # Add some noise\n",
        "        priority_score += np.random.normal(0, 0.1)\n",
        "        priority_score = np.clip(priority_score, 0, 1)\n",
        "        \n",
        "        # Map to priority class\n",
        "        if priority_score < 0.35:\n",
        "            priority = \"low\"\n",
        "        elif priority_score < 0.65:\n",
        "            priority = \"medium\"\n",
        "        else:\n",
        "            priority = \"high\"\n",
        "        \n",
        "        # Store features\n",
        "        data[\"alert_type\"].append(alert_type)\n",
        "        data[\"source_ip_risk\"].append(round(source_ip_risk, 4))\n",
        "        data[\"user_risk_score\"].append(round(user_risk_score, 4))\n",
        "        data[\"failed_login_count_24h\"].append(int(failed_logins))\n",
        "        data[\"geo_impossible_travel\"].append(int(geo_impossible))\n",
        "        data[\"asset_criticality\"].append(asset_crit)\n",
        "        data[\"historical_false_positive_rate\"].append(round(hist_fp_rate, 4))\n",
        "        data[\"priority\"].append(priority)\n",
        "    \n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "    \n",
        "    # Save to CSV\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"✓ Dataset saved to {csv_path}\")\n",
        "    print(f\"\\nDataset shape: {df.shape}\")\n",
        "    print(f\"\\nPriority distribution:\")\n",
        "    print(df[\"priority\"].value_counts())\n",
        "    print(f\"\\nFirst few rows:\")\n",
        "    print(df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Data and Split into Train/Validation Sets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv(csv_path)\n",
        "print(f\"Loaded dataset: {df.shape[0]} samples, {df.shape[1]} features\")\n",
        "\n",
        "# Separate features and target\n",
        "feature_columns = [\n",
        "    \"alert_type\",\n",
        "    \"source_ip_risk\",\n",
        "    \"user_risk_score\",\n",
        "    \"failed_login_count_24h\",\n",
        "    \"geo_impossible_travel\",\n",
        "    \"asset_criticality\",\n",
        "    \"historical_false_positive_rate\"\n",
        "]\n",
        "\n",
        "X = df[feature_columns]\n",
        "y = df[\"priority\"]\n",
        "\n",
        "print(f\"\\nFeatures: {list(X.columns)}\")\n",
        "print(f\"Target: priority\")\n",
        "print(f\"\\nTarget distribution:\")\n",
        "print(y.value_counts().sort_index())\n",
        "\n",
        "# Split into train and validation sets (stratified)\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "print(f\"\\n✓ Train set: {X_train.shape[0]} samples\")\n",
        "print(f\"✓ Validation set: {X_val.shape[0]} samples\")\n",
        "print(f\"\\nTrain target distribution:\")\n",
        "print(y_train.value_counts().sort_index())\n",
        "print(f\"\\nValidation target distribution:\")\n",
        "print(y_val.value_counts().sort_index())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Build Preprocessing Pipeline and Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define feature groups\n",
        "numeric_features = [\n",
        "    \"source_ip_risk\",\n",
        "    \"user_risk_score\",\n",
        "    \"failed_login_count_24h\",\n",
        "    \"historical_false_positive_rate\"\n",
        "]\n",
        "\n",
        "categorical_features = [\n",
        "    \"alert_type\",\n",
        "    \"geo_impossible_travel\",\n",
        "    \"asset_criticality\"\n",
        "]\n",
        "\n",
        "print(\"Numeric features:\", numeric_features)\n",
        "print(\"Categorical features:\", categorical_features)\n",
        "\n",
        "# Build preprocessing pipeline\n",
        "# Use sparse=False for compatibility (works in all sklearn versions)\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", StandardScaler(), numeric_features),\n",
        "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse=False), categorical_features)\n",
        "    ],\n",
        "    remainder=\"drop\"\n",
        ")\n",
        "\n",
        "# Build full pipeline: preprocessor + model\n",
        "model = HistGradientBoostingClassifier(\n",
        "    random_state=42,\n",
        "    max_iter=100,\n",
        "    learning_rate=0.1\n",
        ")\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    (\"preprocessor\", preprocessor),\n",
        "    (\"classifier\", model)\n",
        "])\n",
        "\n",
        "print(\"\\n✓ Pipeline created:\")\n",
        "print(pipeline)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Train Model and Evaluate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the model\n",
        "print(\"Training model...\")\n",
        "pipeline.fit(X_train, y_train)\n",
        "print(\"✓ Training complete!\")\n",
        "\n",
        "# Make predictions on validation set\n",
        "y_val_pred = pipeline.predict(X_val)\n",
        "y_val_proba = pipeline.predict_proba(X_val)\n",
        "\n",
        "# Compute metrics\n",
        "classification_rep = classification_report(\n",
        "    y_val, y_val_pred,\n",
        "    output_dict=True,\n",
        "    target_names=sorted(y.unique())\n",
        ")\n",
        "\n",
        "conf_matrix = confusion_matrix(\n",
        "    y_val, y_val_pred,\n",
        "    labels=sorted(y.unique())\n",
        ")\n",
        "\n",
        "# Print metrics\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"VALIDATION SET METRICS\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_val, y_val_pred, target_names=sorted(y.unique())))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(f\"Labels: {sorted(y.unique())}\")\n",
        "print(conf_matrix)\n",
        "print(\"\\n\" + \"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Save Model, Metadata, and Metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the trained classifier to access classes_\n",
        "classifier = pipeline.named_steps[\"classifier\"]\n",
        "classes = classifier.classes_.tolist()\n",
        "\n",
        "# Create model metadata\n",
        "model_meta = {\n",
        "    \"features\": {\n",
        "        \"numeric\": numeric_features,\n",
        "        \"categorical\": categorical_features\n",
        "    },\n",
        "    \"target\": \"priority\",\n",
        "    \"classes\": classes,\n",
        "    \"training_samples\": int(len(X_train)),\n",
        "    \"validation_samples\": int(len(X_val)),\n",
        "    \"description\": \"HistGradientBoostingClassifier for predicting alert priority (low/medium/high) based on SOC alert features\"\n",
        "}\n",
        "\n",
        "# Save model\n",
        "model_path = MODELS_DIR / \"model.joblib\"\n",
        "joblib.dump(pipeline, model_path)\n",
        "print(f\"✓ Model saved to {model_path}\")\n",
        "\n",
        "# Save model metadata\n",
        "meta_path = MODELS_DIR / \"model_meta.json\"\n",
        "with open(meta_path, \"w\") as f:\n",
        "    json.dump(model_meta, f, indent=2)\n",
        "print(f\"✓ Model metadata saved to {meta_path}\")\n",
        "\n",
        "# Create metrics dictionary (convert numpy types to native Python)\n",
        "metrics = {\n",
        "    \"classification_report\": {\n",
        "        k: {\n",
        "            k2: float(v2) if isinstance(v2, (np.integer, np.floating)) else v2\n",
        "            for k2, v2 in v.items()\n",
        "        } if isinstance(v, dict) else float(v) if isinstance(v, (np.integer, np.floating)) else v\n",
        "        for k, v in classification_rep.items()\n",
        "    },\n",
        "    \"confusion_matrix\": conf_matrix.tolist()\n",
        "}\n",
        "\n",
        "# Save metrics\n",
        "metrics_path = METRICS_DIR / \"metrics.json\"\n",
        "with open(metrics_path, \"w\") as f:\n",
        "    json.dump(metrics, f, indent=2)\n",
        "print(f\"✓ Metrics saved to {metrics_path}\")\n",
        "\n",
        "print(\"\\n✓ All artifacts saved successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Summary: Model Metadata and Metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and display model metadata\n",
        "print(\"=\"*60)\n",
        "print(\"MODEL METADATA\")\n",
        "print(\"=\"*60)\n",
        "with open(meta_path, \"r\") as f:\n",
        "    loaded_meta = json.load(f)\n",
        "print(json.dumps(loaded_meta, indent=2))\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MAIN METRICS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Load and display key metrics\n",
        "with open(metrics_path, \"r\") as f:\n",
        "    loaded_metrics = json.load(f)\n",
        "\n",
        "# Display per-class metrics\n",
        "print(\"\\nPer-class Performance:\")\n",
        "for class_name in sorted(y.unique()):\n",
        "    if class_name in loaded_metrics[\"classification_report\"]:\n",
        "        metrics_dict = loaded_metrics[\"classification_report\"][class_name]\n",
        "        print(f\"\\n{class_name.upper()}:\")\n",
        "        print(f\"  Precision: {metrics_dict.get('precision', 0):.4f}\")\n",
        "        print(f\"  Recall:    {metrics_dict.get('recall', 0):.4f}\")\n",
        "        print(f\"  F1-Score:  {metrics_dict.get('f1-score', 0):.4f}\")\n",
        "        print(f\"  Support:   {int(metrics_dict.get('support', 0))}\")\n",
        "\n",
        "# Display overall metrics\n",
        "overall = loaded_metrics[\"classification_report\"].get(\"weighted avg\", {})\n",
        "print(f\"\\nOverall (Weighted Average):\")\n",
        "print(f\"  Precision: {overall.get('precision', 0):.4f}\")\n",
        "print(f\"  Recall:    {overall.get('recall', 0):.4f}\")\n",
        "print(f\"  F1-Score:  {overall.get('f1-score', 0):.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Training complete! Model ready for deployment.\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
