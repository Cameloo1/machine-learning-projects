{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# News Headline Sentiment Classification\n",
        "\n",
        "End-to-end workflow that turns raw financial news headlines for large-cap tickers into Bullish / Neutral / Bearish sentiment labels, compares linear models, and surfaces the most influential tokens.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction\n",
        "\n",
        "- Fetch fresh headlines for multiple tickers via NewsAPI\n",
        "- Manually label ~250 rows (`data/labeled_headlines.csv`) as Bullish / Bearish / Neutral\n",
        "- Train Logistic Regression and Linear SVM models on TF-IDF text + ticker one-hot features\n",
        "- Evaluate using accuracy + F1, visualize confusion, extract top driver tokens, and package artifacts for sharing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> ðŸ“Œ **Manual labeling requirement**\n",
        ">\n",
        "> 1. Run Step 1 to generate `data/raw_headlines.csv`.\n",
        "> 2. Open that file, add a `sentiment` column with values in {Bullish, Neutral, Bearish} for roughly 250 rows.\n",
        "> 3. Save it as `data/labeled_headlines.csv`, then continue with the notebook.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from dotenv import load_dotenv\n",
        "from scipy import sparse\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Resolve project root so we can import from src/\n",
        "PROJECT_ROOT = Path.cwd().resolve()\n",
        "if not (PROJECT_ROOT / \"src\").exists():\n",
        "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
        "sys.path.append(str(PROJECT_ROOT))\n",
        "\n",
        "load_dotenv(PROJECT_ROOT / \".env\")\n",
        "\n",
        "from src.config import (  # noqa: E402\n",
        "    CLASS_LABELS,\n",
        "    FIGURES_DIR,\n",
        "    LABELED_DATA_PATH,\n",
        "    MODEL_ARTIFACT_PATHS,\n",
        "    MODEL_COMPARISON_CSV,\n",
        "    MODEL_COMPARISON_MD,\n",
        "    RAW_DATA_PATH,\n",
        "    REPORTS_DIR,\n",
        "    TICKERS,\n",
        ")\n",
        "from src.data_fetch import collect_all_headlines  # noqa: E402\n",
        "from src.explain import (  # noqa: E402\n",
        "    get_feature_names,\n",
        "    get_top_tokens_per_class,\n",
        "    save_top_tokens_csv,\n",
        "    save_top_tokens_plot,\n",
        ")\n",
        "from src.preprocess import (  # noqa: E402\n",
        "    build_vectorizers,\n",
        "    clean_text,\n",
        "    load_labeled_data,\n",
        "    transform_features,\n",
        ")\n",
        "from src.train import (  # noqa: E402\n",
        "    evaluate_model,\n",
        "    train_linear_svc,\n",
        "    train_logistic_regression,\n",
        ")\n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", 120)\n",
        "sns.set_theme(style=\"whitegrid\", context=\"talk\")\n",
        "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
        "REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Configured tickers:\", TICKERS)\n",
        "print(\"Raw data path:\", RAW_DATA_PATH)\n",
        "print(\"Labeled data path:\", LABELED_DATA_PATH)\n",
        "print(\"Figures dir:\", FIGURES_DIR)\n",
        "print(\"Reports dir:\", REPORTS_DIR)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1 â€“ Fetch raw headlines (one-time)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the cell below whenever you need a fresh batch of unlabeled headlines. It attempts NewsAPI first, then automatically falls back to the manual scraper (Google News + Yahoo Finance) if the API key is missing, invalid, rate-limited, or if you force manual mode. Either way, the output is `data/raw_headlines.csv` with the same schema. Skip if you already have a labeled dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> ðŸ’¡ To force manual scraping (skip the API entirely), set the environment variable before running the cell:\n",
        ">\n",
        "> ```python\n",
        "> import os\n",
        "> os.environ[\"USE_MANUAL_SCRAPE\"] = \"1\"\n",
        "> ```\n",
        ">\n",
        "> Leave it unset to let the code try NewsAPI first and fall back automatically on failure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "raw_df = None\n",
        "try:\n",
        "    raw_df = collect_all_headlines()\n",
        "except Exception as exc:\n",
        "    print(f\"[notebook] Automatic collection failed: {exc}\")\n",
        "    if RAW_DATA_PATH.exists():\n",
        "        raw_df = pd.read_csv(RAW_DATA_PATH)\n",
        "        print(f\"Loaded cached raw headlines from {RAW_DATA_PATH}\")\n",
        "\n",
        "if raw_df is not None:\n",
        "    display(raw_df.head())\n",
        "    print(f\"Total raw headlines: {len(raw_df):,}\")\n",
        "    print(raw_df['ticker'].value_counts())\n",
        "else:\n",
        "    print(\n",
        "        \"No raw headlines available yet. Provide NEWS_API_KEY or enable manual scraping and rerun.\"\n",
        "    )\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> âœ‹ **Pause here:** Open `data/raw_headlines.csv`, label ~250 rows with `sentiment` in {Bullish, Neutral, Bearish}, and save as `data/labeled_headlines.csv`. Continue once labeling is complete.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2 â€“ Load labeled data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not LABELED_DATA_PATH.exists():\n",
        "    raise FileNotFoundError(\n",
        "        f\"Labeled dataset not found at {LABELED_DATA_PATH}. Create it before proceeding.\"\n",
        "    )\n",
        "\n",
        "labeled_df = load_labeled_data()\n",
        "print(f\"Labeled rows: {len(labeled_df):,}\")\n",
        "display(labeled_df.head())\n",
        "\n",
        "class_counts = labeled_df[\"sentiment\"].value_counts().reindex(CLASS_LABELS, fill_value=0)\n",
        "ax = class_counts.plot(kind=\"bar\", color=[\"#1b7837\", \"#f0ad4e\", \"#d73027\"], figsize=(6, 4))\n",
        "ax.set_ylabel(\"Count\")\n",
        "ax.set_title(\"Class distribution\")\n",
        "plt.show()\n",
        "\n",
        "min_ratio = class_counts.min() / class_counts.sum()\n",
        "if min_ratio < 0.15:\n",
        "    print(\"âš ï¸  Consider labeling more data for the minority class to balance training.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3 â€“ Preprocess + feature engineering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tfidf_vectorizer, ticker_encoder = build_vectorizers(labeled_df)\n",
        "X, y = transform_features(labeled_df, tfidf_vectorizer, ticker_encoder)\n",
        "print(f\"Feature matrix shape: {X.shape}\")\n",
        "print(f\"Label distribution: {dict(zip(*np.unique(y, return_counts=True)))}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4 â€“ Train/validation split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X,\n",
        "    y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y,\n",
        ")\n",
        "\n",
        "print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n",
        "train_counts = dict(zip(*np.unique(y_train, return_counts=True)))\n",
        "test_counts = dict(zip(*np.unique(y_test, return_counts=True)))\n",
        "print(\"Train label counts:\", train_counts)\n",
        "print(\"Test label counts:\", test_counts)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5 â€“ Train & evaluate models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "logreg_model = train_logistic_regression(X_train, y_train)\n",
        "logreg_metrics = evaluate_model(\n",
        "    logreg_model,\n",
        "    X_train,\n",
        "    y_train,\n",
        "    X_test,\n",
        "    y_test,\n",
        "    model_name=\"Logistic Regression\",\n",
        "    figures_dir=FIGURES_DIR,\n",
        ")\n",
        "logreg_metrics\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "linearsvc_model = train_linear_svc(X_train, y_train)\n",
        "linearsvc_metrics = evaluate_model(\n",
        "    linearsvc_model,\n",
        "    X_train,\n",
        "    y_train,\n",
        "    X_test,\n",
        "    y_test,\n",
        "    model_name=\"Linear SVC\",\n",
        "    figures_dir=FIGURES_DIR,\n",
        ")\n",
        "linearsvc_metrics\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6 â€“ Explainability (top tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feature_names = get_feature_names(tfidf_vectorizer, ticker_encoder)\n",
        "\n",
        "logreg_top_tokens = get_top_tokens_per_class(logreg_model, feature_names, top_k=15)\n",
        "logreg_plot_path = save_top_tokens_plot(\n",
        "    logreg_top_tokens,\n",
        "    model_name=\"Logistic Regression\",\n",
        "    figures_dir=FIGURES_DIR,\n",
        ")\n",
        "logreg_csv_path = save_top_tokens_csv(\n",
        "    logreg_top_tokens,\n",
        "    model_name=\"Logistic Regression\",\n",
        "    reports_dir=REPORTS_DIR,\n",
        ")\n",
        "print(\"Logistic Regression explainability saved to:\")\n",
        "print(\" â€¢\", logreg_plot_path)\n",
        "print(\" â€¢\", logreg_csv_path)\n",
        "\n",
        "pd.DataFrame(logreg_top_tokens[\"Bullish\"], columns=[\"feature\", \"weight\"]).head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "linearsvc_top_tokens = get_top_tokens_per_class(linearsvc_model, feature_names, top_k=15)\n",
        "linearsvc_plot_path = save_top_tokens_plot(\n",
        "    linearsvc_top_tokens,\n",
        "    model_name=\"Linear SVC\",\n",
        "    figures_dir=FIGURES_DIR,\n",
        ")\n",
        "linearsvc_csv_path = save_top_tokens_csv(\n",
        "    linearsvc_top_tokens,\n",
        "    model_name=\"Linear SVC\",\n",
        "    reports_dir=REPORTS_DIR,\n",
        ")\n",
        "print(\"Linear SVC explainability saved to:\")\n",
        "print(\" â€¢\", linearsvc_plot_path)\n",
        "print(\" â€¢\", linearsvc_csv_path)\n",
        "\n",
        "pd.DataFrame(linearsvc_top_tokens[\"Bearish\"], columns=[\"feature\", \"weight\"]).head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokens_dict_to_frame(tokens_dict, model_name):\n",
        "    rows = []\n",
        "    for label, tuples in tokens_dict.items():\n",
        "        for feature, weight in tuples:\n",
        "            rows.append({\"model\": model_name, \"class\": label, \"feature\": feature, \"weight\": weight})\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "explain_df = pd.concat(\n",
        "    [\n",
        "        tokens_dict_to_frame(logreg_top_tokens, \"Logistic Regression\"),\n",
        "        tokens_dict_to_frame(linearsvc_top_tokens, \"Linear SVC\"),\n",
        "    ],\n",
        "    ignore_index=True,\n",
        ")\n",
        "explain_df.head(15)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7 â€“ Compare models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "metrics_df = pd.DataFrame([logreg_metrics, linearsvc_metrics])\n",
        "metrics_df\n",
        "\n",
        "metrics_df.to_csv(MODEL_COMPARISON_CSV, index=False)\n",
        "\n",
        "logreg_cm = confusion_matrix(y_test, logreg_model.predict(X_test), labels=CLASS_LABELS)\n",
        "linearsvc_cm = confusion_matrix(y_test, linearsvc_model.predict(X_test), labels=CLASS_LABELS)\n",
        "\n",
        "\n",
        "def describe_confusion(cm):\n",
        "    cm_copy = cm.copy()\n",
        "    np.fill_diagonal(cm_copy, 0)\n",
        "    max_idx = np.unravel_index(np.argmax(cm_copy), cm_copy.shape)\n",
        "    return CLASS_LABELS[max_idx[0]], CLASS_LABELS[max_idx[1]], int(cm_copy[max_idx])\n",
        "\n",
        "logreg_conf = describe_confusion(logreg_cm)\n",
        "linearsvc_conf = describe_confusion(linearsvc_cm)\n",
        "\n",
        "best_macro = metrics_df.loc[metrics_df[\"macro_f1\"].idxmax()]\n",
        "best_weighted = metrics_df.loc[metrics_df[\"weighted_f1\"].idxmax()]\n",
        "\n",
        "summary_lines = [\n",
        "    f\"- Macro F1 leader: {best_macro['model_name']} ({best_macro['macro_f1']:.3f}).\",\n",
        "    f\"- Weighted F1 leader: {best_weighted['model_name']} ({best_weighted['weighted_f1']:.3f}).\",\n",
        "    f\"- Largest confusion (LogReg): {logreg_conf[0]} â†’ {logreg_conf[1]} ({logreg_conf[2]} cases).\",\n",
        "    f\"- Largest confusion (Linear SVC): {linearsvc_conf[0]} â†’ {linearsvc_conf[1]} ({linearsvc_conf[2]} cases).\",\n",
        "    \"- Both models remain interpretable thanks to linear weights over TF-IDF + ticker features.\",\n",
        "]\n",
        "\n",
        "markdown_table = metrics_df.to_markdown(index=False, floatfmt=\".3f\")\n",
        "REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "with open(MODEL_COMPARISON_MD, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"# Model Comparison\\n\\n\")\n",
        "    f.write(markdown_table + \"\\n\\n\")\n",
        "    f.write(\"## Highlights\\n\")\n",
        "    for line in summary_lines:\n",
        "        f.write(line + \"\\n\")\n",
        "\n",
        "print(\"Saved model comparison to:\")\n",
        "print(\" â€¢\", MODEL_COMPARISON_CSV)\n",
        "print(\" â€¢\", MODEL_COMPARISON_MD)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8 â€“ Inference helper\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_registry = {\n",
        "    \"Logistic Regression\": {\"model\": logreg_model, \"metrics\": logreg_metrics},\n",
        "    \"Linear SVC\": {\"model\": linearsvc_model, \"metrics\": linearsvc_metrics},\n",
        "}\n",
        "best_model_name = max(\n",
        "    model_registry.keys(),\n",
        "    key=lambda name: model_registry[name][\"metrics\"][\"macro_f1\"],\n",
        ")\n",
        "print(f\"Using {best_model_name} for inference by default.\")\n",
        "\n",
        "\n",
        "def predict_sentiment(headline: str, ticker: str, model_name: str = best_model_name) -> str:\n",
        "    cleaned = clean_text(headline)\n",
        "    text_vec = tfidf_vectorizer.transform([cleaned])\n",
        "    ticker_vec = ticker_encoder.transform([[ticker.upper()]])\n",
        "    X_sample = sparse.hstack([text_vec, sparse.csr_matrix(ticker_vec)])\n",
        "    model = model_registry[model_name][\"model\"]\n",
        "    return model.predict(X_sample)[0]\n",
        "\n",
        "\n",
        "sample_inputs = [\n",
        "    (\"Apple shares jump after crushing earnings and boosting dividends\", \"AAPL\"),\n",
        "    (\"Tesla faces recall and regulatory probe over autopilot crashes\", \"TSLA\"),\n",
        "    (\"Microsoft trades sideways ahead of mixed macro data\", \"MSFT\"),\n",
        "]\n",
        "for headline, ticker in sample_inputs:\n",
        "    print(f\"{ticker} | {headline}\\n -> {predict_sentiment(headline, ticker)}\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9 â€“ Save artifacts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "models_dir = MODEL_ARTIFACT_PATHS[\"logreg_model\"].parent\n",
        "models_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "joblib.dump(logreg_model, MODEL_ARTIFACT_PATHS[\"logreg_model\"])\n",
        "joblib.dump(linearsvc_model, MODEL_ARTIFACT_PATHS[\"linearsvc_model\"])\n",
        "joblib.dump(tfidf_vectorizer, MODEL_ARTIFACT_PATHS[\"tfidf_vectorizer\"])\n",
        "joblib.dump(ticker_encoder, MODEL_ARTIFACT_PATHS[\"ticker_encoder\"])\n",
        "\n",
        "print(\"Saved artifacts:\")\n",
        "for name, path in MODEL_ARTIFACT_PATHS.items():\n",
        "    print(f\" â€¢ {name}: {path}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "** Wrap up **"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
